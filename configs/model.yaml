backbone_name: 'gpt2' # change to 'stabilityai/stablelm-2-12b' once the loop works
revision: null
use_8bit: false
seed: 42
deterministic: true

trainer:
  backend: auto # auto|cuda|dml|cpu
  amp: false # AMP only on CUDA; leave false for DML
  deterministic: true
  seed: 42
  checkpoint_dir: './checkpoints'

special_tokens:
  user: '<|user|>'
  assistant: '<|assistant|>'
  system: '<|system|>'
  end: '<|endoftext|>'

sft:
  max_seq_len: 1024
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  weight_decay: 0.0
  num_train_epochs: 1
  warmup_ratio: 0.03
  fp16: false # set true for CUDA only
  bf16: false
  gradient_checkpointing: false
  eval_steps: 200
  save_steps: 200
  logging_steps: 50
  early_stopping_patience: 3
  output_dir: 'artifacts/sft'
